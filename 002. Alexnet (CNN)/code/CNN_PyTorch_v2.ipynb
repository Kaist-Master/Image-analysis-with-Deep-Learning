{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Colab Setting \\n# Google Drive Access Authorization  \\nfrom google.colab import drive\\ndrive.mount(\\'/content/gdrive/\\')\\n\\n# Path Setting\\npath = \"/content/gdrive/My Drive/cifar-10/\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Colab Setting \n",
    "# Google Drive Access Authorization  \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "# Path Setting\n",
    "path = \"/content/gdrive/My Drive/cifar-10/\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'D:/Google Drive/cifar-10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED SETTING \n",
    "import random\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 0\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CIFARDataset(BaseDataset):\n",
    "    def __init__(self, path, data, transform = False, train = True):\n",
    "        \"\"\"\n",
    "        train_files : train file list \n",
    "        is_test_or_not : test or not \n",
    "        is_transform : True augmentation \n",
    "        \"\"\"\n",
    "        self.path = path \n",
    "        self.train = train\n",
    "        self.labels = data                  \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지가 있는 파일의 경로를 설정하고 불러올 이미지의 이름을 저장 (id가 1번인 이미지의 파일명은 1.jpg)\n",
    "        img_name = self.path + str(self.labels.iloc[idx, 0])\n",
    "        # 이미지를 열어서 \n",
    "        image_file = Image.open(img_name + '.png')\n",
    "\n",
    "        if self.transform:\n",
    "            torchvision_transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)), \n",
    "                # transforms.RandomCrop(224),\n",
    "                # transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image_file = torchvision_transform(image_file)\n",
    "        else:\n",
    "            torchvision_transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)), \n",
    "                # transforms.RandomCrop(224),\n",
    "                # transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image_file = torchvision_transform(image_file)\n",
    "            \n",
    "        if self.train:\n",
    "            label = self.labels.iloc[idx, 1]\n",
    "            return image_file, label\n",
    "        else:\n",
    "            return image_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels = 3,\n",
    "                                        out_channels = 64,\n",
    "                                        kernel_size = 3,\n",
    "                                        stride = 1,\n",
    "                                        padding = 1),\n",
    "                            nn.BatchNorm2d(64),\n",
    "                            nn.ReLU())\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(in_channels = 64,\n",
    "                                        out_channels = 128,\n",
    "                                        kernel_size = 3,\n",
    "                                        stride = 1,\n",
    "                                        padding = 1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(in_channels = 128,\n",
    "                                        out_channels = 128,\n",
    "                                        kernel_size = 3,\n",
    "                                        stride = 1,\n",
    "                                        padding = 1),\n",
    "                            nn.ReLU())\n",
    "\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(in_channels = 128,\n",
    "                                        out_channels = 128,\n",
    "                                        kernel_size = 3,\n",
    "                                        stride = 1,\n",
    "                                        padding = 1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                            )   \n",
    "        \n",
    "        # 32 -> max pooling 2 times -> 8 \n",
    "        self.layer5 = nn.Flatten()\n",
    "    \n",
    "        self.layer6 = nn.Sequential(nn.Linear(8 * 8 * 128, 256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Dropout(0.5))\n",
    "                  \n",
    "        self.layer7 = nn.Sequential(nn.Linear(256, 256),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "              \n",
    "        self.fc = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.cuda.set_device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filepath + 'trainLabels.csv')\n",
    "\n",
    "# data의 값이 문자열로 되어있기에 수치형으로 변경 \n",
    "class2idx = {}\n",
    "for i, j in enumerate(data['label'].unique()):\n",
    "    class2idx[j] = i\n",
    "\n",
    "idx2class = {}\n",
    "for i, j in class2idx.items():\n",
    "    idx2class[j] = i\n",
    "\n",
    "data['label'] = data['label'].apply(lambda x: class2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb80a9cb48c40699b0cf5191ed28520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10 Train Loss: 14.425495 Train Accuracy: 0.420625 Valid Accuracy: 0.591000\n",
      "Epoch    2/10 Train Loss: 7.934278 Train Accuracy: 0.612075 Valid Accuracy: 0.685800\n",
      "Epoch    3/10 Train Loss: 6.232967 Train Accuracy: 0.686825 Valid Accuracy: 0.714800\n",
      "Epoch    4/10 Train Loss: 5.453641 Train Accuracy: 0.735900 Valid Accuracy: 0.725700\n",
      "Epoch    5/10 Train Loss: 4.680569 Train Accuracy: 0.771950 Valid Accuracy: 0.751000\n",
      "Epoch    6/10 Train Loss: 3.955085 Train Accuracy: 0.796425 Valid Accuracy: 0.743700\n",
      "Epoch    7/10 Train Loss: 3.465817 Train Accuracy: 0.818475 Valid Accuracy: 0.753700\n",
      "Epoch    8/10 Train Loss: 2.924621 Train Accuracy: 0.838600 Valid Accuracy: 0.753700\n",
      "Epoch    9/10 Train Loss: 2.856680 Train Accuracy: 0.854000 Valid Accuracy: 0.757800\n",
      "Epoch   10/10 Train Loss: 2.331471 Train Accuracy: 0.868500 Valid Accuracy: 0.765100\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f0ad6a7a3f4541adf9aaf15d568725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10 Train Loss: 14.292286 Train Accuracy: 0.432475 Valid Accuracy: 0.561800\n",
      "Epoch    2/10 Train Loss: 8.210514 Train Accuracy: 0.621000 Valid Accuracy: 0.681400\n",
      "Epoch    3/10 Train Loss: 6.293219 Train Accuracy: 0.692500 Valid Accuracy: 0.722200\n",
      "Epoch    4/10 Train Loss: 5.543118 Train Accuracy: 0.734300 Valid Accuracy: 0.740000\n",
      "Epoch    5/10 Train Loss: 4.858045 Train Accuracy: 0.772650 Valid Accuracy: 0.739200\n",
      "Epoch    6/10 Train Loss: 3.929193 Train Accuracy: 0.797025 Valid Accuracy: 0.756500\n",
      "Epoch    7/10 Train Loss: 3.447669 Train Accuracy: 0.822025 Valid Accuracy: 0.758000\n",
      "Epoch    8/10 Train Loss: 3.145616 Train Accuracy: 0.838300 Valid Accuracy: 0.767700\n",
      "Epoch    9/10 Train Loss: 2.701858 Train Accuracy: 0.852225 Valid Accuracy: 0.761600\n",
      "Epoch   10/10 Train Loss: 2.449179 Train Accuracy: 0.869675 Valid Accuracy: 0.765300\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c0e3ba16784a7f83782d0a648a1596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10 Train Loss: 14.689694 Train Accuracy: 0.406175 Valid Accuracy: 0.567300\n",
      "Epoch    2/10 Train Loss: 8.503443 Train Accuracy: 0.616025 Valid Accuracy: 0.693000\n",
      "Epoch    3/10 Train Loss: 6.058511 Train Accuracy: 0.701900 Valid Accuracy: 0.729600\n",
      "Epoch    4/10 Train Loss: 5.206500 Train Accuracy: 0.749125 Valid Accuracy: 0.737400\n",
      "Epoch    5/10 Train Loss: 4.319948 Train Accuracy: 0.780975 Valid Accuracy: 0.754700\n",
      "Epoch    6/10 Train Loss: 3.809959 Train Accuracy: 0.811450 Valid Accuracy: 0.769200\n",
      "Epoch    7/10 Train Loss: 3.181496 Train Accuracy: 0.832825 Valid Accuracy: 0.775400\n",
      "Epoch    8/10 Train Loss: 2.744412 Train Accuracy: 0.856425 Valid Accuracy: 0.768800\n",
      "Epoch    9/10 Train Loss: 2.391010 Train Accuracy: 0.867625 Valid Accuracy: 0.758800\n",
      "Epoch   10/10 Train Loss: 2.143937 Train Accuracy: 0.883825 Valid Accuracy: 0.765300\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ff3904f5346da8a730b6a6d7befb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10 Train Loss: 14.204767 Train Accuracy: 0.438325 Valid Accuracy: 0.608700\n",
      "Epoch    2/10 Train Loss: 7.966471 Train Accuracy: 0.629675 Valid Accuracy: 0.665600\n",
      "Epoch    3/10 Train Loss: 5.974961 Train Accuracy: 0.701425 Valid Accuracy: 0.726000\n",
      "Epoch    4/10 Train Loss: 4.972634 Train Accuracy: 0.745050 Valid Accuracy: 0.731400\n",
      "Epoch    5/10 Train Loss: 4.498719 Train Accuracy: 0.777475 Valid Accuracy: 0.755100\n",
      "Epoch    6/10 Train Loss: 3.953755 Train Accuracy: 0.805875 Valid Accuracy: 0.760300\n",
      "Epoch    7/10 Train Loss: 3.283971 Train Accuracy: 0.831275 Valid Accuracy: 0.765900\n",
      "Epoch    8/10 Train Loss: 2.879429 Train Accuracy: 0.848700 Valid Accuracy: 0.766100\n",
      "Epoch    9/10 Train Loss: 2.501077 Train Accuracy: 0.866350 Valid Accuracy: 0.759000\n",
      "Epoch   10/10 Train Loss: 2.314623 Train Accuracy: 0.883475 Valid Accuracy: 0.758900\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39233c3fc1dc40c08ecd6e7e27edc64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10 Train Loss: 14.314605 Train Accuracy: 0.432150 Valid Accuracy: 0.575700\n",
      "Epoch    2/10 Train Loss: 7.728228 Train Accuracy: 0.627250 Valid Accuracy: 0.687500\n",
      "Epoch    3/10 Train Loss: 6.105800 Train Accuracy: 0.694700 Valid Accuracy: 0.697100\n",
      "Epoch    4/10 Train Loss: 5.482239 Train Accuracy: 0.736300 Valid Accuracy: 0.739200\n",
      "Epoch    5/10 Train Loss: 4.814955 Train Accuracy: 0.771400 Valid Accuracy: 0.759300\n",
      "Epoch    6/10 Train Loss: 3.964044 Train Accuracy: 0.795750 Valid Accuracy: 0.733300\n",
      "Epoch    7/10 Train Loss: 3.715030 Train Accuracy: 0.820250 Valid Accuracy: 0.764400\n",
      "Epoch    8/10 Train Loss: 3.253132 Train Accuracy: 0.835250 Valid Accuracy: 0.766900\n",
      "Epoch    9/10 Train Loss: 2.747804 Train Accuracy: 0.854725 Valid Accuracy: 0.767300\n",
      "Epoch   10/10 Train Loss: 2.314112 Train Accuracy: 0.868625 Valid Accuracy: 0.762700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold \n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "nb_epochs = 10\n",
    "\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(kf.split(range(0, 50000))):\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    best_valid = 0\n",
    "    \n",
    "    train_loader = DataLoader(CIFARDataset(filepath + 'train/',data = data.iloc[tr_idx] , transform = True, train = True), batch_size=64, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(CIFARDataset(filepath + 'train/',data = data.iloc[val_idx] , transform = False, train = True), batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = DeepCNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(0, nb_epochs)):\n",
    "        # train 학습 \n",
    "        train_loss = 0\n",
    "        correct_tr = 0\n",
    "        correct_val = 0\n",
    "        \n",
    "        model.train()\n",
    "        for idx, (train_batch, label) in enumerate(train_loader):\n",
    "            train_batch, label = train_batch.to(dev), label.to(dev)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            prediction = model(train_batch)\n",
    "            loss = criterion(prediction, label)    \n",
    "            loss.backward()\n",
    "            train_loss += loss.item() / (idx+1)\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(prediction, 1)\n",
    "            correct_tr += (predicted == label).sum()\n",
    "            \n",
    "        # valid 검증 \n",
    "        valid_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (valid_batch, label) in enumerate(valid_loader):\n",
    "                valid_batch, label = valid_batch.to(dev), label.to(dev)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                prediction = model(valid_batch)\n",
    "                loss = criterion(prediction, label)    \n",
    "                valid_loss += loss.item() / (idx+1)\n",
    "                _, predicted = torch.max(prediction, 1)\n",
    "                correct_val += (predicted == label).sum()\n",
    "\n",
    "        \n",
    "        if epoch % 1 == 0: \n",
    "            print('Epoch {:4d}/{} Train Loss: {:.6f} Train Accuracy: {:.6f} Valid Accuracy: {:.6f}'.format(epoch+1, nb_epochs, train_loss, \n",
    "                                                                                correct_tr.detach().numpy() / len(train_loader.dataset),\n",
    "                                                                                correct_val.detach().numpy() / len(valid_loader.dataset)))\n",
    "        train_accuracy.append(correct_tr.detach().numpy() / len(train_loader.dataset))\n",
    "        valid_accuracy.append(correct_val.detach().numpy() / len(valid_loader.dataset))\n",
    "        \n",
    "        # 모델의 스코어가 가장 높은 모델을 저장 \n",
    "        # 단, 원래는 Validation set으로 진행해야 하지만 분석의 편의상 Train으로 진행 \n",
    "        if (best_valid == None or best_valid < correct_val):\n",
    "                best_valid = correct_val\n",
    "                torch.save(model, filepath + '/savedmodel/{}_cifar10-cnn_v2.pth'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filepath + 'sampleSubmission.csv')\n",
    "\n",
    "test_dataloader = DataLoader(CIFARDataset(filepath + 'test/', data = data, transform = False, train=False), batch_size=1, shuffle=False, num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9102c3cf3480490592bdec6838068fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c308666a986413f83dcb6d50d5c0742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed2b4a66d73423a95c64f1af40335dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efabed823c745219e299e97b62cd49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437b57bde74f421fb9e994fd6b4306f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch_kfold_model ={1:'1_cifar10-cnn_v2.pth',\n",
    "                    2:'2_cifar10-cnn_v2.pth',\n",
    "                    3:'3_cifar10-cnn_v2.pth',\n",
    "                    4:'4_cifar10-cnn_v2.pth',\n",
    "                    5:'5_cifar10-cnn_v2.pth'}\n",
    "preds_all = []\n",
    "for i in range(1,6):\n",
    "    print(f\"{i} FOLD Predict\")\n",
    "    model_name = torch_kfold_model[i]\n",
    "    best_model = torch.load(filepath + f'/savedmodel/{model_name}')\n",
    "    best_model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, feature in tqdm_notebook(enumerate(test_dataloader)):\n",
    "            # 32*32 : image size \n",
    "            predict = best_model(feature.to(dev))\n",
    "            _, predict = torch.max(predict, 1)\n",
    "            results.append(idx2class[predict.detach().numpy()[0]])\n",
    "    # preds = np.concatenate(results,axis=0)\n",
    "    preds_all.append(results.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission = pd.read_csv(filepath + 'sampleSubmission.csv')\n",
    "sampleSubmission['label'] = results\n",
    "\n",
    "label = pd.DataFrame(np.array(preds_all)).mode()\n",
    "sampleSubmission['label'] = label.loc[0]\n",
    "sampleSubmission.to_csv(filepath + \"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
