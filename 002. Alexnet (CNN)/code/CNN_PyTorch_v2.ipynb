{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN PyTorch_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv2mcDsYAse+0xTWXh7IzM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ElbzgrtOr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "8df3b4a9-ecdc-433f-cc0f-bdd272f51d9c"
      },
      "source": [
        "# Colab Setting \n",
        "# Google Drive Access Authorization  \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-337fc44acf5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Google Drive Access Authorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;34m': timeout during initial read of root folder; for more info: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             'https://research.google.com/colaboratory/faq.html#drive-timeout')\n\u001b[0;32m--> 237\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed: timeout during initial read of root folder; for more info: https://research.google.com/colaboratory/faq.html#drive-timeout"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EJyytrttTPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path Setting\n",
        "path = \"/content/gdrive/My Drive/cifar-10/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh7wLlAjta3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets,transforms\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIEroA-tpc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SEED SETTING \n",
        "import random\n",
        "import os \n",
        "import numpy as np\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 0\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpcaIIAbtsLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "from PIL import Image\n",
        "\n",
        "class CIFARDataset(BaseDataset):\n",
        "    def __init__(self, path, transform = False, tr_index = None):\n",
        "        \"\"\"\n",
        "        train_files : train file list \n",
        "        is_test_or_not : test or not \n",
        "        is_transform : True augmentation \n",
        "        \"\"\"\n",
        "        self.path = path\n",
        "        if tr_index is None\n",
        "            self.trainLabels = pd.read_csv(\"/content/gdrive/My Drive/cifar-10/\" + 'trainLabels.csv')\n",
        "        else:\n",
        "            self.trainLabels = pd.read_csv(\"/content/gdrive/My Drive/cifar-10/\" + 'trainLabels.csv').iloc[tr_index]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 이미지가 있는 파일의 경로를 설정하고 불러올 이미지의 이름을 저장 (id가 1번인 이미지의 파일명은 1.jpg)\n",
        "        img_name = os.path.join(self.path, str(self.trainLabels.iloc[idx, 0]))\n",
        "        # 이미지를 열어서 \n",
        "        image = Image.open(img_name + '.png')\n",
        "        if transform:\n",
        "            # albumentations : PyTorch augmentation 도와주는 도구 \n",
        "            albumentations_transforms = albumentations.Compose([\n",
        "                albumentations.Resize(32, 32),\n",
        "                albumentations.RandomCrop(224, 224),\n",
        "                albumentations.HorizontalFlip(), # Same with transforms.RandomHorizontalFlip()\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "            image = albumentations_transforms(image)\n",
        "\n",
        "        labels = self.trainLabels.iloc[idx, 1]\n",
        "        \n",
        "        return image, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoK3jsvfu6a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels = 3,\n",
        "                                        out_channels = 64,\n",
        "                                        kernel_size = 3,\n",
        "                                        stride = 1,\n",
        "                                        padding = 1),\n",
        "                            nn.ReLU())\n",
        "        \n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(in_channels = 64,\n",
        "                                        out_channels = 128,\n",
        "                                        kernel_size = 3,\n",
        "                                        stride = 1,\n",
        "                                        padding = 1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        \n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(in_channels = 128,\n",
        "                                        out_channels = 128,\n",
        "                                        kernel_size = 3,\n",
        "                                        stride = 1,\n",
        "                                        padding = 1),\n",
        "                            nn.ReLU())\n",
        "\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(in_channels = 128,\n",
        "                                        out_channels = 128,\n",
        "                                        kernel_size = 3,\n",
        "                                        stride = 1,\n",
        "                                        padding = 1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
        "                            )   \n",
        "        \n",
        "        # 32 -> max pooling 2 times -> 8 \n",
        "        self.layer5 = nn.Flatten()\n",
        "             \n",
        "        self.layer6 = nn.Sequential(nn.Linear(8 * 8 * 128, 256),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.5))\n",
        "                  \n",
        "        self.layer7 = nn.Sequential(nn.Linear(256, 256),\n",
        "                            nn.ReLU()\n",
        "                            )\n",
        "              \n",
        "        self.fc = nn.Linear(256, 10)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.layer6(x)\n",
        "        x = self.layer7(x)\n",
        "        x = self.fc(x)\n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci7Fw-SHu8CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepCNN().to(device)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNChY2X-u9aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u2M0mqgt01m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import KFold \n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "kf = KFold(5, shuffle=True, random_state=0)\n",
        "nb_epochs = 10\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(kf):\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_loader = DataLoader(CIFARDataset(path + 'train', transform = True, tr_idx), batch_size=64, shuffle=True, num_workers=0)\n",
        "    valid_loader = DataLoader(CIFARDataset(path + 'train', transform = False, val_idx), batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "    for epoch in tqdm_notebook(range(0, nb_epochs)):\n",
        "        # train 학습 \n",
        "        train_loss = 0\n",
        "        model.train()\n",
        "        for idx, train_batch, label in enumerate(train_loader):\n",
        "            train_batch, label = train_batch.to(dev), label.to(dev)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            prediction = model(train_batch)\n",
        "            loss = criterion(prediction, label)    \n",
        "            loss.backward()\n",
        "            train_loss += loss.item() / (idx+1)\n",
        "            optimizer.step()\n",
        "\n",
        "        # valid 검증 \n",
        "        valid_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "        for idx, valid_batch, label in enumerate(valid_loader):\n",
        "            valid_batch, label = valid_batch.to(dev), label.to(dev)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            prediction = model(valid_batch)\n",
        "            loss = criterion(prediction, label)    \n",
        "            valid_loss += loss.item() / (idx+1)\n",
        "\n",
        "        \n",
        "        if epoch % 1 == 0: \n",
        "            print('Epoch {:4d}/{} Train Loss: {:.6f} Test Loss: {:.6f}'.format(epoch+1, nb_epochs, train_loss, valid_loss))\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        # 모델의 스코어가 가장 높은 모델을 저장 \n",
        "        # 단, 원래는 Validation set으로 진행해야 하지만 분석의 편의상 Train으로 진행 \n",
        "        if (best_valid == None or best_valid < valid_loss):\n",
        "                best_valid = valid_loss\n",
        "                torch.save(model.state_dict(), path + '/savedmodel/{}_cifar10-cnn_v2.pth'.format(i+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD0wGsKV2YrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = DataLoader(CIFARDataset(path + 'test', transform = False), batch_size=1, shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mogj8yivvEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch_kfold_model ={1:'1_cifar10-cnn_v2.pth',\n",
        "                    2:'2_cifar10-cnn_v2.pth',\n",
        "                    3:'3_cifar10-cnn_v2.pth',\n",
        "                    4:'4_cifar10-cnn_v2.pth',\n",
        "                    5:'5_cifar10-cnn_v2.pth'}\n",
        "preds_all = []\n",
        "for i in range(1,6):\n",
        "    print(f\"{i} FOLD Predict\")\n",
        "    model_name = torch_kfold_model[i]\n",
        "    best_model = torch.load(f'savedmodel/{model_name}')\n",
        "    best_model.eval()\n",
        "    results = []\n",
        "    for batch_idx, (feature, target) in tqdm(enumerate(test_dataloader)):\n",
        "        # 32*32 : image size \n",
        "        results.append(best_model.predict(feature.cuda()).view(-1,32*32).cpu().numpy())\n",
        "    preds = np.concatenate(results,axis=0)\n",
        "    preds_all.append(preds.copy())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}